"""
æ™ºèƒ½æœç´¢åŠ©æ‰‹ - åŸºäº LangGraph + Tavily API çš„çœŸå®æœç´¢ç³»ç»Ÿ
1. ç†è§£ç”¨æˆ·éœ€æ±‚
2. ä½¿ç”¨Tavily APIçœŸå®æœç´¢ä¿¡æ¯  
3. ç”ŸæˆåŸºäºæœç´¢ç»“æœçš„å›ç­”
"""

import asyncio
from typing import TypedDict, Annotated
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import InMemorySaver
import os
from dotenv import load_dotenv
from tavily import TavilyClient

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å®šä¹‰çŠ¶æ€ç»“æ„
class SearchState(TypedDict):
    messages: Annotated[list, add_messages]
    user_query: str        # ç”¨æˆ·æŸ¥è¯¢
    search_query: str      # ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢
    search_results: str    # Tavilyæœç´¢ç»“æœ
    final_answer: str      # æœ€ç»ˆç­”æ¡ˆ
    step: str             # å½“å‰æ­¥éª¤

# åˆå§‹åŒ–æ¨¡å‹å’ŒTavilyå®¢æˆ·ç«¯
llm = ChatOpenAI(
    model=os.getenv("LLM_MODEL_ID", "gpt-4o-mini"),
    api_key=os.getenv("LLM_API_KEY"),
    base_url=os.getenv("LLM_BASE_URL", "https://api.openai.com/v1"),
    temperature=0.7
)

# åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

def understand_query_node(state: SearchState) -> SearchState:
    """æ­¥éª¤1ï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢å…³é”®è¯"""
    
    # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
    user_message = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg.content
            break
    
    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"

è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ
2. ç”Ÿæˆæœ€é€‚åˆæœç´¢çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼‰

æ ¼å¼ï¼š
ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""

    response = llm.invoke([SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼Œè´Ÿè´£ç†è§£ç”¨æˆ·æ„å›¾å¹¶æå‡ºæœç´¢å…³é”®è¯ã€‚"),
        HumanMessage(content=understand_prompt)])
    
    # æå–æœç´¢å…³é”®è¯
    response_text = response.content
    search_query = user_message  # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢
    
    if "æœç´¢è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢è¯ï¼š")[1].strip()
    elif "æœç´¢å…³é”®è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢å…³é”®è¯ï¼š")[1].strip()
    
    return {
        "user_query": response.content,
        "search_query": search_query,
        "step": "understood",
        "messages": [AIMessage(content=f"æˆ‘ç†è§£æ‚¨çš„éœ€æ±‚ï¼š{response.content}")]
    }

def tavily_search_node(state: SearchState) -> SearchState:
    """æ­¥éª¤2ï¼šä½¿ç”¨Tavily APIè¿›è¡ŒçœŸå®æœç´¢"""
    
    search_query = state["search_query"]
    
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢: {search_query}")
        
        # è°ƒç”¨Tavilyæœç´¢API
        response = tavily_client.search(
            query=search_query,
            search_depth="basic",
            include_answer=True,
            include_raw_content=False,
            max_results=5
        )
        
        # å¤„ç†æœç´¢ç»“æœ
        search_results = ""
        
        # ä¼˜å…ˆä½¿ç”¨Tavilyçš„ç»¼åˆç­”æ¡ˆ
        if response.get("answer"):
            search_results = f"ç»¼åˆç­”æ¡ˆï¼š\n{response['answer']}\n\n"
        
        # æ·»åŠ å…·ä½“çš„æœç´¢ç»“æœ
        if response.get("results"):
            search_results += "ç›¸å…³ä¿¡æ¯ï¼š\n"
            for i, result in enumerate(response["results"][:3], 1):
                title = result.get("title", "")
                content = result.get("content", "")
                url = result.get("url", "")
                search_results += f"{i}. {title}\n{content}\næ¥æºï¼š{url}\n\n"
        
        if not search_results:
            search_results = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        return {
            "search_results": search_results,
            "step": "searched",
            "messages": [AIMessage(content=f"âœ… æœç´¢å®Œæˆï¼æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ï¼Œæ­£åœ¨ä¸ºæ‚¨æ•´ç†ç­”æ¡ˆ...")]
        }
        
    except Exception as e:
        error_msg = f"æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        
        return {
            "search_results": f"æœç´¢å¤±è´¥ï¼š{error_msg}",
            "step": "search_failed",
            "messages": [AIMessage(content="âŒ æœç´¢é‡åˆ°é—®é¢˜ï¼Œæˆ‘å°†åŸºäºå·²æœ‰çŸ¥è¯†ä¸ºæ‚¨å›ç­”")]
        }

def generate_answer_node(state: SearchState) -> SearchState:
    """æ­¥éª¤3ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
    if state["step"] == "search_failed":
        # å¦‚æœæœç´¢å¤±è´¥ï¼ŒåŸºäºLLMçŸ¥è¯†å›ç­”
        fallback_prompt = f"""æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

è¯·æä¾›ä¸€ä¸ªæœ‰ç”¨çš„å›ç­”ï¼Œå¹¶è¯´æ˜è¿™æ˜¯åŸºäºå·²æœ‰çŸ¥è¯†çš„å›ç­”ã€‚"""
        
        response = llm.invoke([ SystemMessage(content="ä½ æ˜¯ä¸€ä½çŸ¥è¯†ä¸°å¯Œçš„åŠ©æ‰‹ã€‚"),
            HumanMessage(content=fallback_prompt)])
        
        return {
            "final_answer": response.content,
            "step": "completed",
            "messages": [AIMessage(content=response.content)]
        }
    
    # åŸºäºæœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
    answer_prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å®Œæ•´ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

æœç´¢ç»“æœï¼š
{state['search_results']}

è¯·è¦æ±‚ï¼š
1. ç»¼åˆæœç´¢ç»“æœï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”
2. å¦‚æœæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œæä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆæˆ–ä»£ç 
3. å¼•ç”¨é‡è¦ä¿¡æ¯çš„æ¥æº
4. å›ç­”è¦ç»“æ„æ¸…æ™°ã€æ˜“äºç†è§£
5. å¦‚æœæœç´¢ç»“æœä¸å¤Ÿå®Œæ•´ï¼Œè¯·è¯´æ˜å¹¶æä¾›è¡¥å……å»ºè®®"""

    response = llm.invoke([
        SystemMessage(content="ä½ æ˜¯ä¸€ä½çŸ¥è¯†æ•´åˆåŠ©æ‰‹ï¼ŒåŸºäºçœŸå®æœç´¢ç»“æœå›ç­”é—®é¢˜ã€‚"),
        HumanMessage(content=answer_prompt)
    ])
    
    return {
        "final_answer": response.content,
        "step": "completed",
        "messages": [AIMessage(content=response.content)]
    }

# æ„å»ºæœç´¢å·¥ä½œæµ
def create_search_assistant():
    workflow = StateGraph(SearchState)
    
    # æ·»åŠ ä¸‰ä¸ªèŠ‚ç‚¹
    workflow.add_node("understand", understand_query_node)
    workflow.add_node("search", tavily_search_node)
    workflow.add_node("answer", generate_answer_node)
    
    # è®¾ç½®çº¿æ€§æµç¨‹
    workflow.add_edge(START, "understand")
    workflow.add_edge("understand", "search")
    workflow.add_edge("search", "answer")
    workflow.add_edge("answer", END)
    
    # ç¼–è¯‘å›¾
    memory = InMemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    return app

async def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ™ºèƒ½æœç´¢åŠ©æ‰‹"""
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("TAVILY_API_KEY"):
        print("âŒ é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®TAVILY_API_KEY")
        return
    
    app = create_search_assistant()
    
    print("ğŸ” æ™ºèƒ½æœç´¢åŠ©æ‰‹å¯åŠ¨ï¼")
    print("æˆ‘ä¼šä½¿ç”¨Tavily APIä¸ºæ‚¨æœç´¢æœ€æ–°ã€æœ€å‡†ç¡®çš„ä¿¡æ¯")
    print("æ”¯æŒå„ç§é—®é¢˜ï¼šæ–°é—»ã€æŠ€æœ¯ã€çŸ¥è¯†é—®ç­”ç­‰")
    print("(è¾“å…¥ 'quit' é€€å‡º)\n")
    
    session_count = 0
    
    while True:
        user_input = input("ğŸ¤” æ‚¨æƒ³äº†è§£ä»€ä¹ˆ: ").strip()
        
        if user_input.lower() in ['quit', 'q', 'é€€å‡º', 'exit']:
            print("æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼ğŸ‘‹")
            break
        
        if not user_input:
            continue
        
        session_count += 1
        #thread_id ç”¨æ¥æ ‡è¯†ä¸€æ¬¡å¯¹è¯/ä¼šè¯çš„å”¯ä¸€çº¿ç¨‹ ID
        #å°†æœ¬æ¬¡è¿è¡Œå½’å…¥ä¸€ä¸ªä¼šè¯çº¿ç¨‹ search-session-{session_count} ï¼Œä¾¿äºåŒºåˆ†ä¸åŒè½®æ¬¡æˆ–ä¸åŒç”¨æˆ·çš„å¯¹è¯
        #å¦‚æœå›¾é…ç½®äº†â€œæ£€æŸ¥ç‚¹/è®°å¿†â€å­˜å‚¨ï¼ˆå¦‚ MemorySaver ã€ SqliteSaver ï¼‰ï¼Œ thread_id ä¼šä½œä¸ºé”®ï¼Œç”¨äºåŠ è½½/ä¿å­˜è¯¥ä¼šè¯çš„çŠ¶æ€ï¼Œå®ç°è·¨è¯·æ±‚ç»­æ¥ä¸å›æ”¾ã€‚
        config = {"configurable": {"thread_id": f"search-session-{session_count}"}}
        
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_query": "",
            "search_query": "",
            "search_results": "",
            "final_answer": "",
            "step": "start"
        }
        
        try:
            print("\n" + "="*60)
            
            # æ‰§è¡Œå·¥ä½œæµ ä¸€è¾¹è¿è¡Œå·¥ä½œæµã€ä¸€è¾¹å®æ—¶æ‰“å°æ¯ä¸ªèŠ‚ç‚¹äº§ç”Ÿçš„æœ€æ–°æ¨¡å‹å›å¤ã€‚
            # ä»¥æµæ¨¡å¼è¿è¡Œå›¾ï¼ŒèŠ‚ç‚¹ä¸€å®Œæˆå°±ç«‹åˆ»äº§å‡ºä¸€æ¬¡ outputï¼Œè€Œä¸æ˜¯ç­‰æ‰€æœ‰èŠ‚ç‚¹å®Œæˆã€‚
            async for output in app.astream(initial_state, config=config):
                # è¿”å›å½“å‰è¿™ä¸€æ­¥æ‰€æœ‰å®ŒæˆèŠ‚ç‚¹çš„æ›´æ–°ï¼Œé”®æ˜¯ node_name ï¼Œå€¼æ˜¯è¯¥èŠ‚ç‚¹çš„çŠ¶æ€åˆ‡ç‰‡ node_outputã€‚
                for node_name, node_output in output.items():
                    if "messages" in node_output and node_output["messages"]:
                        #å–è¯¥èŠ‚ç‚¹æœ€æ–°çš„ä¸€æ¡æ¶ˆæ¯
                        latest_message = node_output["messages"][-1]
                        #ä»…å½“æœ€æ–°æ¶ˆæ¯æ˜¯å¤§æ¨¡å‹å›å¤ï¼ˆ AIMessage ï¼‰æ‰æ‰“å°
                        if isinstance(latest_message, AIMessage):
                            if node_name == "understand":
                                print(f"ğŸ§  ç†è§£é˜¶æ®µ: {latest_message.content}")
                            elif node_name == "search":
                                print(f"ğŸ” æœç´¢é˜¶æ®µ: {latest_message.content}")
                            elif node_name == "answer":
                                print(f"\nğŸ’¡ æœ€ç»ˆå›ç­”:\n{latest_message.content}")
            
            print("\n" + "="*60 + "\n")
        
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚\n")

if __name__ == "__main__":
    asyncio.run(main())